{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a708a5d7",
   "metadata": {},
   "source": [
    "## Crear &#129303; Huggin Face dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e8909e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "323b360d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesadas 200,070 mil fotos de las cuales 106 están corruptas\r"
     ]
    }
   ],
   "source": [
    "df = pd.read_json(r'dataset/photos.json', lines=True)\n",
    "df = df.drop(columns = ['business_id', 'caption'])\n",
    "contador = 0\n",
    "discarded = 0\n",
    "for img in df.photo_id.tolist():\n",
    "    if (contador%30 == 0) and (contador > 0):\n",
    "        print('Procesadas {:7,} mil fotos de las cuales {} están corruptas'.format(contador, discarded), end='\\r');\n",
    "    contador += 1\n",
    "    try:\n",
    "        img = Image.open('dataset/photos/'+img+'.jpg')\n",
    "        img.verify()\n",
    "    except:\n",
    "        # file is corrupt\n",
    "        df = df.drop(df.loc[df.photo_id == img].index)\n",
    "        discarded += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d34b2ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe('photo_id', 'label') contiene todas las fotos válidas del dataset\n",
    "pickle.dump(df, open('checkpoints/valid.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f9f6e0",
   "metadata": {},
   "source": [
    "## &#128190; Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ee95de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7832d6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pickle.load(open('checkpoints/valid.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e210b500",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorias = df.label.value_counts().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "23741b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {'train' : {},\n",
    "          'test': {},\n",
    "          'validate': {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "580c784a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for categoria in categorias:\n",
    "    slice_80 = int(len(df.loc[df.label == categoria]) * 0.8)\n",
    "    slice_10 = int(len(df.loc[df.label == categoria]) * 0.1)\n",
    "    dataset['train'].update({categoria: df.loc[df.label == categoria][0:slice_80]})\n",
    "    dataset['test'].update({categoria: df.loc[df.label == categoria][slice_80:slice_80+slice_10]})\n",
    "    dataset['validate'].update({categoria: df.loc[df.label == categoria][slice_80+slice_10:]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bd299c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42 # La solución de Douglas Adams para todo\n",
    "max_photos = 2500\n",
    "\n",
    "subset_max_photos = {\n",
    "    'train': int(max_photos * 0.8),\n",
    "    'test': int(max_photos * 0.1)\n",
    "}\n",
    "\n",
    "subset_max_photos.update({\n",
    "    'validate': max_photos - subset_max_photos['train'] - subset_max_photos['test'] # las que quedan\n",
    "})\n",
    "\n",
    "# hacer que los subsets tengan max_photos imagenes\n",
    "for subset in ['train', 'test', 'validate']:\n",
    "    for categoria in categorias:\n",
    "        if len(dataset[subset][categoria]) == subset_max_photos[subset]: # tenemos el numero correcto\n",
    "            pass            \n",
    "        elif len(dataset[subset][categoria]) > subset_max_photos[subset]: # demasiadas fotos - resample\n",
    "            dataset[subset][categoria] = dataset[subset][categoria].sample(n=subset_max_photos[subset],\n",
    "                                                                           random_state=random_state,\n",
    "                                                                           replace=False, # solo 1x cada foto\n",
    "                                                                           ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "460434de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/yelp removed\n",
      "dataset/yelp created\n",
      "dataset/yelp/train created\n",
      "dataset/yelp/train/food created\n",
      "dataset/yelp/train/inside created\n",
      "dataset/yelp/train/outside created\n",
      "dataset/yelp/train/drink created\n",
      "dataset/yelp/train/menu created\n",
      "dataset/yelp/test created\n",
      "dataset/yelp/test/food created\n",
      "dataset/yelp/test/inside created\n",
      "dataset/yelp/test/outside created\n",
      "dataset/yelp/test/drink created\n",
      "dataset/yelp/test/menu created\n",
      "dataset/yelp/validate created\n",
      "dataset/yelp/validate/food created\n",
      "dataset/yelp/validate/inside created\n",
      "dataset/yelp/validate/outside created\n",
      "dataset/yelp/validate/drink created\n",
      "dataset/yelp/validate/menu created\n"
     ]
    }
   ],
   "source": [
    "dataset_origin_path='dataset/photos'\n",
    "dataset_output_path='dataset/yelp'\n",
    "dataset_json_file='dataset/photos.json'\n",
    "\n",
    "import shutil\n",
    "\n",
    "# crear estructura de carpetas\n",
    "if os.path.isdir(dataset_output_path):\n",
    "    shutil.rmtree(dataset_output_path, ignore_errors=False, onerror=None) # remove tree\n",
    "    print('{} removed'.format(dataset_output_path))\n",
    "    \n",
    "os.mkdir(dataset_output_path)\n",
    "print('{} created'.format(dataset_output_path))\n",
    "    \n",
    "for subset in ['train', 'test', 'validate']:\n",
    "    os.mkdir(dataset_output_path + '/' + subset)\n",
    "    print('{} created'.format(dataset_output_path + '/' + subset))\n",
    "    for categoria in categorias:\n",
    "        os.mkdir(dataset_output_path + '/' + subset + '/' + categoria)\n",
    "        print('{} created'.format(dataset_output_path + '/' + subset + '/' + categoria))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ad0ca96f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/yelp/train/food - copiadas   2,000 fotos\n",
      "dataset/yelp/train/inside - copiadas   2,000 fotos\n",
      "dataset/yelp/train/outside - copiadas   2,000 fotos\n",
      "dataset/yelp/train/drink - copiadas   2,000 fotos\n",
      "dataset/yelp/train/menu - copiadas   1,342 fotos\n",
      "dataset/yelp/train/menu - generadas     658 fotos\n",
      "dataset/yelp/test/food - copiadas     250 fotos\n",
      "dataset/yelp/test/inside - copiadas     250 fotos\n",
      "dataset/yelp/test/outside - copiadas     250 fotos\n",
      "dataset/yelp/test/drink - copiadas     250 fotos\n",
      "dataset/yelp/test/menu - copiadas     167 fotos\n",
      "dataset/yelp/test/menu - generadas      83 fotos\n",
      "dataset/yelp/validate/food - copiadas     250 fotos\n",
      "dataset/yelp/validate/inside - copiadas     250 fotos\n",
      "dataset/yelp/validate/outside - copiadas     250 fotos\n",
      "dataset/yelp/validate/drink - copiadas     250 fotos\n",
      "dataset/yelp/validate/menu - copiadas     169 fotos\n",
      "dataset/yelp/validate/menu - generadas      81 fotos\n"
     ]
    }
   ],
   "source": [
    "# copiar o generar imagenes\n",
    "\n",
    "from torchvision import transforms as T\n",
    "\n",
    "random_state = 42 # La solución de Douglas Adams para todo\n",
    "\n",
    "from torch import manual_seed\n",
    "manual_seed(random_state)\n",
    "\n",
    "import random\n",
    "random.seed(random_state)\n",
    "\n",
    "max_photos = 2500\n",
    "\n",
    "subset_max_photos = {\n",
    "    'train': int(max_photos * 0.8),\n",
    "    'test': int(max_photos * 0.1)\n",
    "}\n",
    "\n",
    "subset_max_photos.update({\n",
    "    'validate': max_photos - subset_max_photos['train'] - subset_max_photos['test'] # las que quedan\n",
    "})\n",
    "\n",
    "pipeline = T.Compose([\n",
    "    T.ColorJitter(brightness=.5, hue=.3),\n",
    "    T.RandomAffine(degrees=(30, 70), scale=(0.9, 1.1))\n",
    "])\n",
    "\n",
    "for subset in ['train', 'test', 'validate']:\n",
    "    for categoria in categorias:\n",
    "        # copiar\n",
    "        contador = 0\n",
    "        for img in dataset[subset][categoria].photo_id:\n",
    "            shutil.copyfile(dataset_origin_path + '/' + img + '.jpg',\n",
    "                            dataset_output_path + '/' + subset + '/' + categoria + '/' + img + '.jpg')\n",
    "            if (contador%30 == 0) and (contador > 0):\n",
    "                print('{} - copiadas {:7,} fotos'.format(dataset_output_path + '/' + subset + '/' + categoria,\n",
    "                                                         contador),\n",
    "                      end='\\r');\n",
    "            contador += 1\n",
    "        print('{} - copiadas {:7,} fotos'.format(dataset_output_path + '/' + subset + '/' + categoria,\n",
    "                                                 contador));\n",
    "    \n",
    "        # generar imagenes si necesario\n",
    "        if len(dataset[subset][categoria]) < subset_max_photos[subset]: # faltan imagenes\n",
    "\n",
    "            # determinar cuantas imagenes tendremos que generar\n",
    "            n_missing = subset_max_photos[subset] - len(dataset[subset][categoria])\n",
    "            \n",
    "            # seleccionar imagenes que vamos usar como base\n",
    "            df_base_images = dataset[subset][categoria].sample(n=n_missing,\n",
    "                                                               random_state=random_state,\n",
    "                                                               replace=False, # solo 1x cada foto\n",
    "                                                               ignore_index=True)\n",
    "            # abrir original > transformar > guardar imagen transformada > añadir nueva foto al subset\n",
    "            contador = 0\n",
    "            for img_name in df_base_images.photo_id:\n",
    "                original_image_path = dataset_origin_path + '/' + img_name + '.jpg'\n",
    "                new_image_path = dataset_output_path + '/' + subset + '/' + categoria + '/' + img_name + '.jpg''_tr.jpg'\n",
    "                with Image.open(original_image_path) as img:\n",
    "                    augmented_image = pipeline(img = img)\n",
    "                    augmented_image.save(new_image_path)\n",
    "                                \n",
    "                new_img = pd.DataFrame(data={'photo_id': img_name+'_tr.jpg',\n",
    "                                             'label': df.loc[df.photo_id == img_name].label})\n",
    "                dataset[subset][categoria] = pd.concat([dataset[subset][categoria], new_img])\n",
    "                if (contador%30 == 0) and (contador > 0):\n",
    "                    print('{} - generadas {:7,} fotos'.format(dataset_output_path + '/' + subset + '/' + categoria,\n",
    "                                                             contador),\n",
    "                          end='\\r');\n",
    "                contador += 1\n",
    "\n",
    "            print('{} - generadas {:7,} fotos'.format(dataset_output_path + '/' + subset + '/' + categoria,\n",
    "                                                     contador));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96e3045",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
